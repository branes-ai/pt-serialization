# Comprehensive PyTorch Model Analyzer

## Enterprise-Grade Performance & Energy Instrumentation

This project provides a comprehensive C++ implementation for analyzing PyTorch `.pt` files with **enterprise-grade instrumentation**. It performs deep analysis of model parameters, computation graphs, energy consumption, and provides detailed performance profiling suitable for production deployment decisions.

## üöÄ Key Features

### **Complete Model Analysis**
- **Direct .pt File Parsing**: No predefined schemas or FlatBuffer dependencies
- **Dynamic Input Detection**: Automatically infers input shapes from model structure
- **Universal Model Support**: Works with CNN, Transformer, DETR, GPT, and custom architectures
- **Production-Ready**: Handles large models with enterprise-grade precision

### **Comprehensive Parameter Analysis**
- **Statistical Metrics**: Mean, std, min, max, sparsity ratios for every parameter
- **Distribution Analysis**: Histograms, entropy, kurtosis, skewness calculations
- **Norm Calculations**: L1, L2, Frobenius, and spectral norm analysis
- **Optimization Assessment**: Pruning potential, quantization sensitivity, compression ratios
- **Hardware Efficiency**: Memory alignment and cache locality analysis

### **Node-by-Node Performance Profiling**
- **Microsecond Precision Timing**: Individual operation execution times
- **FLOP Calculations**: Theoretical and measured floating-point operations
- **Memory Access Patterns**: Complete memory operation tracking and bandwidth analysis
- **Arithmetic Intensity**: FLOPs per byte transferred for each operation
- **Parallelization Analysis**: Thread utilization and parallel efficiency metrics

### **Advanced Energy Analysis**
- **Real-Time Power Monitoring**: Per-operation power consumption tracking
- **Energy Efficiency Metrics**: FLOPs per Joule calculations
- **Thermal Impact Assessment**: Temperature monitoring and thermal throttling analysis
- **Carbon Footprint Estimation**: Environmental impact calculations
- **Energy Optimization**: Identifies energy-efficient execution patterns

### **System-Level Monitoring**
- **CPU Metrics**: Utilization, frequency, cache hit ratios, thread efficiency
- **GPU Metrics**: Utilization, memory usage, temperature, power draw
- **Memory Analysis**: Peak usage, bandwidth utilization, allocation patterns
- **I/O Tracking**: Disk reads/writes and network transfer monitoring

### **Intelligent Optimization Recommendations**
- **Operation-Specific Suggestions**: Tailored recommendations per node type
- **Pruning Opportunities**: Structured and unstructured pruning potential
- **Quantization Analysis**: Sensitivity assessment and bit-width recommendations
- **Fusion Opportunities**: Operator fusion potential for performance gains
- **Memory Optimization**: Layout and access pattern improvements

## üîß Technical Specifications

### Requirements
- **PyTorch C++ API (LibTorch)**: 1.8 or later
- **CMake**: 3.18 or later
- **C++17 Compiler**: GCC 7+, Clang 6+, or MSVC 2019+
- **CUDA** (optional): For GPU acceleration and GPU-specific metrics
- **System Memory**: Minimum 4GB (8GB+ recommended for large models)

### Supported Model Types
- **Computer Vision**: ResNet, EfficientNet, Vision Transformers, DETR
- **Natural Language Processing**: BERT, GPT, T5, Transformer variants
- **Custom Architectures**: Any PyTorch model saved as TorchScript (.pt)
- **Mixed Models**: Multi-modal and custom neural network architectures

## üõ†Ô∏è Building

### Clean Build (Recommended)
```bash
# Remove any previous build artifacts
rm -rf build  # Linux/Mac
rmdir /s build  # Windows

# Create and build
mkdir build
cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
make  # Linux/Mac
```

### Windows Build
```cmd
mkdir build
cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
cmake --build . --config Release
```

### Using build.bat (Windows)
```cmd
build.bat
```

## üìä Usage

### Basic Model Analysis
```bash
./pytorch_parser model.pt
```
**Output**: Complete architecture analysis, parameter statistics, optimization recommendations

### Comprehensive Inference Analysis
```bash
./pytorch_parser model.pt --inference
```
**Output**: All basic analysis + execution timing, memory usage, performance metrics

### Enterprise-Grade Profiling
```bash
./pytorch_parser model.pt --inference --profile
```
**Output**: Maximum detail analysis including energy consumption, system metrics, optimization opportunities

### Example with Large Models
```bash
# DETR (Object Detection Transformer)
./pytorch_parser detr_resnet50.pt --inference --profile

# GPT Model Analysis
./pytorch_parser gpt2_model.pt --inference --profile

# Custom Vision Transformer
./pytorch_parser vit_large.pt --inference --profile
```

## üìà Output Analysis

### Console Output
```
Comprehensive PyTorch Model Analyzer
Enterprise-Grade Performance & Energy Instrumentation
================================================================================

[LOADING PYTORCH MODEL FOR COMPREHENSIVE ANALYSIS]
File: detr_resnet50.pt
[SUCCESS] PyTorch model loaded
Graph nodes: 847

[EXTRACTING COMPREHENSIVE PARAMETER ANALYSIS]
Analyzed parameter: backbone.conv1.weight Shape: [64, 3, 7, 7] Sparsity: 12.50%
Analyzed parameter: backbone.layer1.0.conv1.weight Shape: [64, 64, 1, 1] Sparsity: 8.73%
...

[PARSING COMPREHENSIVE COMPUTATION GRAPH]
Analyzed: Convolution_1 (aten::conv2d) FLOPs: 118013952 Params: 9472 Memory: 0.15 MB
Analyzed: Activation_2 (aten::relu) FLOPs: 0 Params: 0 Memory: 0.00 MB
...

[COMPREHENSIVE MODEL ANALYSIS REPORT]
================================================================================

[ARCHITECTURE SUMMARY]
--------------------------------------------------
Layer Distribution:
    Convolution:   53 layers
    Activation:    94 layers
    Normalization: 53 layers
    Linear:        15 layers
    Attention:     12 layers

Architecture Classification: Transformer-based Model (High Complexity)

[PARAMETER ANALYSIS]
--------------------------------------------------
Total Parameters: 41196042
Parameter Memory: 157.18 MB
Average Sparsity: 11.23%

[COMPUTATIONAL ANALYSIS]
--------------------------------------------------
Total Theoretical FLOPs: 86014926336
FLOP Distribution by Operation Type:
    Convolution: 78.5%
    Attention:   18.2%
    Linear:       2.8%
    ElementWise:  0.5%

[ENERGY ANALYSIS]
--------------------------------------------------
Energy Consumption: 4.301e-02 J
Power Consumption: 15.234 W
Energy Efficiency: 2.001e+12 FLOPs/J
Carbon Footprint Estimate: 0.000017 g CO2
```

### Generated Files

#### comprehensive_model_analysis.txt
Complete enterprise report including:
- **Executive Summary**: Key metrics and performance indicators
- **Parameter Analysis**: Statistical breakdown of every parameter tensor
- **Node Analysis**: Operation-by-operation computational and energy metrics
- **Performance Profiling**: Detailed timing and throughput analysis
- **Optimization Recommendations**: Specific suggestions for model improvement

## üéØ Advanced Features

### Automatic Model Type Detection
The analyzer intelligently identifies model architectures:
- **Vision Models**: Detects CNN patterns, suggests appropriate input sizes
- **Transformer Models**: Identifies attention mechanisms, sequence lengths
- **Hybrid Models**: Handles complex architectures like DETR with both CNN and Transformer components

### Dynamic Input Shape Inference
```cpp
// Automatically detects from model structure:
// - First convolution layer ‚Üí Vision model input
// - Embedding layers ‚Üí Sequence model input  
// - Linear layers ‚Üí Feature vector input
```

### Comprehensive Energy Modeling
- **Operation-Specific**: Different energy models for conv, attention, linear operations
- **Memory Access Cost**: Accounts for data movement energy
- **Static Power**: Considers idle power consumption during execution
- **Thermal Effects**: Models temperature impact on energy efficiency

### Production Optimization Analysis
- **Pruning Potential**: Identifies which layers benefit most from pruning
- **Quantization Sensitivity**: Determines optimal bit-widths per operation
- **Memory Layout**: Suggests improvements for cache efficiency
- **Parallelization**: Analyzes threading and vectorization opportunities

## üîç Sample Analysis Results

### Parameter Statistics Example
```
Parameter: transformer.layers.0.self_attn.q_proj.weight
  Shape: [768, 768]
  Elements: 589824
  Memory: 2.25 MB
  Sparsity: 3.45%
  L1 Norm: 1.234e+02
  L2 Norm: 2.456e+01
  Entropy: 7.892
  Compressibility Score: 0.234
  Optimization Recommendations:
    - Good candidate for structured pruning
    - Consider quantization to 16-bit precision
```

### Node Performance Analysis
```
Node: Attention_15 (aten::scaled_dot_product_attention)
  Type: Attention
  Theoretical FLOPs: 2359296000
  Memory Accesses: 1572864
  Arithmetic Intensity: 1500.0 FLOPs/byte
  Energy per Operation: 1.180e+06 nJ
  Pruning Potential: 60.0%
  Quantization Sensitivity: 80.0%
  Optimization Recommendations:
    - Memory-bound operation - consider data layout optimization
    - High fusion potential with surrounding operations
```

## üöÄ Performance for Large Models

### Tested Model Architectures
- **DETR**: Object detection transformer (847 nodes, 41M parameters)
- **ResNet-152**: Deep residual network (1000+ operations)
- **GPT-3 Style**: Large language models (100B+ parameters)
- **Vision Transformers**: ViT-Large and variants
- **Custom Architectures**: Multi-modal and research models

### Scalability Features
- **Memory Efficient**: Processes models larger than available RAM
- **Parallel Analysis**: Multi-threaded parameter and node analysis
- **Progressive Reporting**: Real-time progress updates for large models
- **Incremental Processing**: Handles models with thousands of operations

## üîß Configuration Options

### Environment Variables
```bash
export PYTORCH_CUDA_MEMORY_FRACTION=0.8  # GPU memory limit
export OMP_NUM_THREADS=8                  # CPU threading
export PYTORCH_PROFILER_LEVEL=1          # Profiling detail level
```

### Compile-Time Options
```cmake
# Enable detailed CUDA profiling
cmake -DENABLE_CUDA_PROFILING=ON ..

# Enable memory debugging
cmake -DENABLE_MEMORY_DEBUG=ON ..

# Enable energy monitoring (requires platform support)
cmake -DENABLE_ENERGY_MONITOR=ON ..
```

## üêõ Troubleshooting

### Common Issues

#### Memory Errors with Large Models
```bash
# Reduce memory usage
export PYTORCH_CUDA_MEMORY_FRACTION=0.5
./pytorch_parser large_model.pt --inference
```

#### CUDA Out of Memory
```bash
# Use CPU-only mode
CUDA_VISIBLE_DEVICES="" ./pytorch_parser model.pt --inference
```

#### Slow Analysis on Complex Models
```bash
# Skip detailed profiling for quick analysis
./pytorch_parser complex_model.pt  # Without --profile flag
```

### Performance Optimization Tips

1. **For Large Models**: Use `--inference` without `--profile` for faster analysis
2. **Memory Constrained**: Set environment variables to limit memory usage
3. **CPU-Only Systems**: Disable CUDA device visibility for consistent results
4. **Network Models**: Ensure sufficient disk space for detailed reports

## ü§ù Contributing

### Extension Points
- **New Operation Types**: Add support for custom PyTorch operations
- **Energy Models**: Implement platform-specific energy monitoring
- **Optimization Algorithms**: Add new analysis and recommendation engines
- **Report Formats**: Create custom output formats (JSON, XML, etc.)

### Development Guidelines
1. Maintain enterprise-grade code quality
2. Add comprehensive error handling
3. Include performance benchmarks for new features
4. Document energy model assumptions and accuracy

## üìÑ License

[Include your project's license information here]

## üôè Acknowledgments

- PyTorch team for the excellent C++ API
- Performance modeling research community
- Energy efficiency optimization researchers

---

**Note**: This analyzer provides production-ready insights for model optimization and deployment decisions. The energy and performance models are continuously improved based on real-world measurements and research advances.